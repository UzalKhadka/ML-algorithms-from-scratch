{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision\n",
    "## Introduction\n",
    "Precision is the fraction of correct positive predictions out of all positive predictions. It is also known as positive predictive value or Sensitivity.\n",
    "$$ Precision = \\frac{TP}{TP + FP} $$\n",
    "where,\n",
    "* TP = True Positive\n",
    "* FP = False Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import numpy as np\n",
    "from warnings import warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case\n",
    "Precision is most useful when we want the prediction to match the actual value as closely as possible. For example, in a medical test, we want to minimize the number of false negatives. In this case, we want to maximize the precision. Because, for example, if we predict that a patient does not have a disease, but he actually has it, then it can be very dangerous for the patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision score function / Sensitivity / Positive Predictive Value\n",
    "# precision = tp / (tp + fp)\n",
    "def precision_score(y_true, y_pred, average='binary', zero_division='warn'):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # for binary classification, we can use the formula straight away\n",
    "    if average == 'binary':\n",
    "        assert len(np.unique(y_true)) == 2, 'y_true should have only 2 unique values i.e. binary classification'\n",
    "        \n",
    "        tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "        fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "        \n",
    "        if tp + fp == 0: # if there are no true positives and false positives, we will get a division by zero error\n",
    "            if zero_division == 'warn':\n",
    "                warn(\"set zero_division to 0 or 1 to avoid this warning. Default is 'warn'\")\n",
    "                return 0 # since x / 0\n",
    "            elif zero_division == 0:\n",
    "                return 0\n",
    "            elif zero_division == 1:\n",
    "                return 1\n",
    "            else:\n",
    "                warn(\"RuntimeWarning: zero_division must be either 0, 1 or 'warn'\")\n",
    "                return 0 # since x / 0\n",
    "        else:\n",
    "            return tp / (tp + fp)\n",
    "    \n",
    "    # for multiclass classification, we need to calculate precision for each class\n",
    "    elif average == 'micro':\n",
    "        tp = np.sum((y_true == y_pred))\n",
    "        fp = np.sum((y_true != y_pred))\n",
    "        \n",
    "        if tp + fp == 0: # if there are no true positives and false positives, we will get a division by zero error\n",
    "            if zero_division == 'warn':\n",
    "                warn(\"set zero_division to 0 or 1 to avoid this warning. Default is 'warn'\")\n",
    "                return 0 # since x / 0\n",
    "            elif zero_division == 0:\n",
    "                return 0\n",
    "            elif zero_division == 1:\n",
    "                return 1\n",
    "            else:\n",
    "                warn(\"RuntimeWarning: zero_division must be either 0, 1 or 'warn'\")\n",
    "                return 0 # since x / 0\n",
    "        else:\n",
    "            return tp / (tp + fp)\n",
    "    \n",
    "    # for multiclass classification, we need to calculate precision for each class and return the mean\n",
    "    elif average == 'macro':\n",
    "        classes = np.unique(y_true)\n",
    "        pre_scores = []\n",
    "        \n",
    "        for c in classes:\n",
    "            tp = np.sum((y_true == c) & (y_pred == c))\n",
    "            fp = np.sum((y_true != c) & (y_pred == c))\n",
    "            \n",
    "            if tp + fp == 0: # if there are no true positives and false positives, we will get a division by zero error\n",
    "                if zero_division == 'warn':\n",
    "                    warn(\"set zero_division to 0 or 1 to avoid this warning. Default is 'warn'\")\n",
    "                    pre_scores.append(0) # since x / 0\n",
    "                elif zero_division == 0:\n",
    "                    pre_scores.append(0)\n",
    "                elif zero_division == 1:\n",
    "                    pre_scores.append(1)\n",
    "                else:\n",
    "                    warn(\"RuntimeWarning: zero_division must be either 0, 1 or 'warn'\")\n",
    "                    pre_scores.append(0) # since x / 0\n",
    "            else:\n",
    "                pre_scores.append(tp / (tp + fp))\n",
    "            \n",
    "        return np.mean(pre_scores)\n",
    "    \n",
    "    # for multiclass classification, we need to calculate precision for each class and return the score for each class\n",
    "    elif average == None:\n",
    "        classes = np.unique(y_true)\n",
    "        pre_scores = []\n",
    "        \n",
    "        for c in classes:\n",
    "            tp = np.sum((y_true == c) & (y_pred == c))\n",
    "            fp = np.sum((y_true != c) & (y_pred == c))\n",
    "            \n",
    "            if tp + fp == 0: # if there are no true positives and false positives, we will get a division by zero error\n",
    "                if zero_division == 'warn':\n",
    "                    warn(\"set zero_division to 0 or 1 to avoid this warning. Default is 'warn'\")\n",
    "                    pre_scores.append(0) # since x / 0\n",
    "                elif zero_division == 0:\n",
    "                    pre_scores.append(0)\n",
    "                elif zero_division == 1:\n",
    "                    pre_scores.append(1)\n",
    "                else:\n",
    "                    warn(\"RuntimeWarning: zero_division must be either 0, 1 or 'warn'\")\n",
    "                    pre_scores.append(0) # since x / 0\n",
    "            else:\n",
    "                pre_scores.append(tp / (tp + fp))\n",
    "            \n",
    "        return np.array(pre_scores)\n",
    "    \n",
    "    # Else, we raise an error\n",
    "    else:\n",
    "        raise ValueError(\"average parameter must be 'binary', 'macro', 'micro', or None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "The below tests are taken from the Scikit Learn documentation. Source: <https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0, 1, 2, 0, 1, 2]\n",
    "y_pred = [0, 2, 1, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2222222222222222"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_true, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "average parameter must be 'binary', 'macro', 'micro', or None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6936\\1732110610.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprecision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'weighted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6936\\3648799943.py\u001b[0m in \u001b[0;36mprecision_score\u001b[1;34m(y_true, y_pred, average, zero_division)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;31m# Else, we raise an error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"average parameter must be 'binary', 'macro', 'micro', or None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: average parameter must be 'binary', 'macro', 'micro', or None"
     ]
    }
   ],
   "source": [
    "precision_score(y_true, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66666667, 0.        , 0.        ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_true, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\predator\\AppData\\Local\\Temp\\ipykernel_6936\\3648799943.py:83: UserWarning: set zero_division to 0 or 1 to avoid this warning. Default is 'warn'\n",
      "  warn(\"set zero_division to 0 or 1 to avoid this warning. Default is 'warn'\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 0.        , 0.        ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [0, 0, 0, 0, 0, 0]\n",
    "precision_score(y_true, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 1.        , 1.        ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_true, y_pred, average=None, zero_division=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
